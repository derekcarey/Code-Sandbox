{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Derek Rendering OpenAi Gym in Colaboratory.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsSGgH2Y0apl"
      },
      "source": [
        "#          _____                _____                    _____                    _____                    _____                    _____          \n",
        "#         /\\    \\              /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\         \n",
        "#        /::\\    \\            /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\        \n",
        "#       /::::\\    \\           \\:::\\    \\              /::::\\    \\              /::::\\    \\              /::::\\    \\               \\:::\\    \\       \n",
        "#      /::::::\\    \\           \\:::\\    \\            /::::::\\    \\            /::::::\\    \\            /::::::\\    \\               \\:::\\    \\      \n",
        "#     /:::/\\:::\\    \\           \\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\               \\:::\\    \\     \n",
        "#    /:::/__\\:::\\    \\           \\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\               \\:::\\    \\    \n",
        "#    \\:::\\   \\:::\\    \\          /::::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\              /::::\\    \\   \n",
        "#  ___\\:::\\   \\:::\\    \\        /::::::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    ____    /::::::\\    \\  \n",
        "# /\\   \\:::\\   \\:::\\    \\      /:::/\\:::\\    \\  /:::/\\:::\\   \\:::\\    \\  /:::/\\:::\\   \\:::\\____\\  /:::/\\:::\\   \\:::\\    \\  /\\   \\  /:::/\\:::\\    \\ \n",
        "#/::\\   \\:::\\   \\:::\\____\\    /:::/  \\:::\\____\\/:::/  \\:::\\   \\:::\\____\\/:::/  \\:::\\   \\:::|    |/:::/  \\:::\\   \\:::\\____\\/::\\   \\/:::/  \\:::\\____\\\n",
        "#\\:::\\   \\:::\\   \\::/    /   /:::/    \\::/    /\\::/    \\:::\\  /:::/    /\\::/   |::::\\  /:::|____|\\::/    \\:::\\  /:::/    /\\:::\\  /:::/    \\::/    /\n",
        "# \\:::\\   \\:::\\   \\/____/   /:::/    / \\/____/  \\/____/ \\:::\\/:::/    /  \\/____|:::::\\/:::/    /  \\/____/ \\:::\\/:::/    /  \\:::\\/:::/    / \\/____/ \n",
        "#  \\:::\\   \\:::\\    \\      /:::/    /                    \\::::::/    /         |:::::::::/    /            \\::::::/    /    \\::::::/    /          \n",
        "#   \\:::\\   \\:::\\____\\    /:::/    /                      \\::::/    /          |::|\\::::/    /              \\::::/    /      \\::::/____/           \n",
        "#    \\:::\\  /:::/    /    \\::/    /                       /:::/    /           |::| \\::/____/               /:::/    /        \\:::\\    \\           \n",
        "#     \\:::\\/:::/    /      \\/____/                       /:::/    /            |::|  ~|                    /:::/    /          \\:::\\    \\          \n",
        "#      \\::::::/    /                                    /:::/    /             |::|   |                   /:::/    /            \\:::\\    \\         \n",
        "#       \\::::/    /                                    /:::/    /              \\::|   |                  /:::/    /              \\:::\\____\\        \n",
        "#        \\::/    /                                     \\::/    /                \\:|   |                  \\::/    /                \\::/    /        \n",
        "#         \\/____/                                       \\/____/                  \\|___|                   \\/____/                  \\/____/    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2"
      },
      "source": [
        "# install dependancies, takes around 45 seconds\n",
        "\n",
        "Rendering Dependancies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ"
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A-1LTSH88EE"
      },
      "source": [
        "Pacman Dependancies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI"
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXSx7hg19TH"
      },
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj"
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5uDYEGtZlkE"
      },
      "source": [
        "Create functions for creating video for replay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D"
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3BGbWOu179M"
      },
      "source": [
        "# Pacman!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRBqt4NEKjtc"
      },
      "source": [
        "\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import os # for creating directories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGEFMfDOzLen"
      },
      "source": [
        "env = wrap_env(gym.make(\"MsPacman-v0\"))\n",
        "s = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pCGO2V9Ztb4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQSuFy2vZ6my"
      },
      "source": [
        "Function for converting code to grayscale (110,90) shape. Credit to Trevor McInroe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DD5YjT4SP-f"
      },
      "source": [
        "def phi(frame):\n",
        "    \"\"\"Expects type batch of tensors (already normed)\"\"\"\n",
        "    # (1) Grayscale\n",
        "    frame = tf.image.rgb_to_grayscale(frame)\n",
        "    \n",
        "    # (2) Downsample\n",
        "    frame = tf.image.resize(frame, size=[110, 90])\n",
        "    \n",
        "    # (3) Crop only the relevant portion\n",
        "    return frame[:, :90,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otHjWiadZ9i1"
      },
      "source": [
        "Display sample image in grayscale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7LTO97-ajrO"
      },
      "source": [
        "example_state = phi(tf.convert_to_tensor([s / 255.]))\n",
        "plt.imshow(tf.squeeze(example_state[0]), cmap='gray')\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K20HbdbrLMpU"
      },
      "source": [
        "action_size = env.action_space.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD3GzQFdaN1g"
      },
      "source": [
        "Set number of episodes to 501 to allow for 500 iterations. Set batch size to 128 and set the neural net training dimensions to (90, 90, 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDIeCZ08LTx-"
      },
      "source": [
        "n_episodes = 501\n",
        "batch_size=128\n",
        "input_dims1 = (90,90, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BmIlXhe9Q89"
      },
      "source": [
        "#check out the pacman action space!\n",
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slIwcmYIafoP"
      },
      "source": [
        "Mount Google drive to save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY08hmEQLvRT"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk86pLs6axTr"
      },
      "source": [
        "Set output directory to Google drive location "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CrzA7bzg9JA"
      },
      "source": [
        "output_dir = '/content/gdrive/My Drive/msds/462/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klpho_tva9Ae"
      },
      "source": [
        "Create directory if it doesn't exist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8VYSanWfI0J"
      },
      "source": [
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07guu6zlbl9Y"
      },
      "source": [
        "Create DQNAgent. This includes creating a neueral net to approximate the Q-value function. Two Conv2d layers are included with 16 and 32 filters followed by a flattening and Dense layer. Output is a linear activation of a output dense layer of size action_size (9). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7LX5lx9L0b-"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Input\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, action_size,input_dims):\n",
        "        self.action_size = action_size\n",
        "        self.input_dims = input_dims\n",
        "        self.memory = deque(maxlen=10000) # double-ended queue; acts like list, but elements can be added/removed from either end\n",
        "        self.gamma = 0.95 # decay or discount rate: enables agent to take into account future actions in addition to the immediate ones, but discounted at this rate\n",
        "        self.epsilon = 1.0 # exploration rate: how much to act randomly; more initially than later due to epsilon decay\n",
        "        self.epsilon_decay = 0.995 # decrease number of random explorations as the agent's performance (hopefully) improves over time\n",
        "        self.epsilon_min = 0.01 # minimum amount of random exploration permitted\n",
        "        self.learning_rate = 0.001 # rate at which NN adjusts models parameters via SGD to reduce cost \n",
        "        self.model = self._build_model() # private method \n",
        "    \n",
        "    def _build_model(self):\n",
        "        # neural net to approximate Q-value function:\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(input_shape=self.input_dims, filters=16, strides=4,kernel_size=(8,8),activation='relu',padding='same')) # 1st hidden layer; states as input\n",
        "        model.add(Conv2D(filters=32, strides=2,kernel_size=(4,4),activation='relu',padding='same'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(256,activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear')) # 9 actions, so 9 output neurons: 0 and 1 (L/R)\n",
        "        model.compile(loss='mse',\n",
        "                      optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "    \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, \n",
        "                            reward, next_state, done)) # list of previous experiences, enabling re-training later\n",
        "\n",
        "    def train(self, batch_size): # method that trains NN with experiences sampled from memory\n",
        "        minibatch = random.sample(self.memory, batch_size) # sample a minibatch from memory\n",
        "        for state, action, reward, next_state, done in minibatch: # extract data for each minibatch sample\n",
        "            target = reward # if done (boolean whether game ended or not, i.e., whether final state or not), then target = reward\n",
        "            if not done: # if not done, then predict future discounted reward\n",
        "                target = (reward + \n",
        "                          self.gamma * # (target) = reward + (discount rate gamma) * \n",
        "                          np.amax(self.model.predict(next_state)[0])) # (maximum target Q based on future action a')\n",
        "            target_f = self.model.predict(state) # approximately map current state to future discounted reward\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0) # single epoch of training with x=state, y=target_f; fit decreases loss btwn target_f and y_hat\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon: # if acting randomly, take random action\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state) # if not acting randomly, predict reward value based on current state\n",
        "        return np.argmax(act_values[0]) # pick the action that will give the highest reward (i.e., go left or right?)\n",
        "    \n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcIfaHHhdcfF"
      },
      "source": [
        "Initialize the agent "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cJoyUSvLbfE"
      },
      "source": [
        "agent = DQNAgent(action_size, input_dims1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkulFNh0gj9Y"
      },
      "source": [
        "scores = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRHs2VV-ZyoP"
      },
      "source": [
        "env = wrap_env(gym.make(\"MsPacman-v0\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J5kbqd7L9ti"
      },
      "source": [
        "\n",
        "\n",
        "for e in range(1, n_episodes): # iterate over episodes of gameplay \n",
        "    state = env.reset()\n",
        "    state = phi(tf.convert_to_tensor([state/255])) # reset state at start of each new episode of the game\n",
        "    done = False\n",
        "    score = 0\n",
        "    while not done: \n",
        "        #env.render()\n",
        "        action = agent.act(state) # action is either 0 or 1 (move cart left or right); decide on one or other here\n",
        "        next_state, reward, done, _ = env.step(action) # agent interacts with env, gets feedback; 4 state data points, e.g., pole angle, cart position        \n",
        "        score = score + reward\n",
        "        next_state = phi(tf.convert_to_tensor([next_state/255]))\n",
        "        #next_state = np.reshape(next_state, [90, 90])\n",
        "        agent.remember(state, action, reward, next_state, done) # remember the previous timestep's state, actions, reward, etc.        \n",
        "        state = next_state # set \"current state\" for upcoming iteration to the current next state        \n",
        "        if done: # if episode ends: \n",
        "            print(\"episode: {}/{}, score: {}, e: {:.2}\" # print the episode's score and agent's epsilon\n",
        "                  .format(e, n_episodes-1, score, agent.epsilon))\n",
        "        scores.append(score)\n",
        "    if len(agent.memory) > batch_size:\n",
        "        agent.train(batch_size) # train the agent by replaying the experiences of the episode\n",
        "    if e % 20 == 0:\n",
        "        agent.save(output_dir + \"weights_latest_train_3\" \n",
        "                   + '{:04d}'.format(e) + \".hdf5\")\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1TB65GRzBf_"
      },
      "source": [
        "#agent = DQNAgent( action_size,  input_dims1)\n",
        "agent.load(output_dir + 'weights_latest_train_30500.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nj5sjsk15IT"
      },
      "source": [
        "env = wrap_env(gym.make(\"MsPacman-v0\"))\n",
        "\n",
        "state = env.reset()\n",
        "state = phi(tf.convert_to_tensor([state/255])) # reset state at start of each new episode of the game\n",
        "done = False\n",
        "score = 0\n",
        "# time represents a frame of the episode; goal is to keep pole upright as long as possible\n",
        "while not done: \n",
        "    env.render()\n",
        "    action = agent.act(state) # action is to move in some direction\n",
        "    next_state, reward, done, _ = env.step(action) # agent interacts with env, gets feedback; \n",
        "    score = score + reward       \n",
        "    next_state = next_state\n",
        "    next_state = phi(tf.convert_to_tensor([next_state/255]))\n",
        "\n",
        "    state = next_state # set \"current state\" for upcoming iteration to the current next state        \n",
        "\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}